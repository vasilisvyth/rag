from typing import List
from langchain import PromptTemplate
from deepeval import evaluate
from deepeval.test_case import LLMTestCase
from langchain_openai import ChatOpenAI
from langchain_core.pydantic_v1 import BaseModel, Field
from model_utils import create_openai_language_model
from metrics import correctness_metric, faithfulness_metric
import sys
import os
from retrieval_utils import retrieve_context_per_question, hyde
from chain_utils import build_chain_with_structured_output

current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.append(parent_dir)

def create_prompt_template(template, input_variables):
    """
    Creates a PromptTemplate object with the specified template and input variables.

    Args:
        template (str): The template string to format the prompt.
        input_variables (List[str]): List of variable names to include in the template.

    Returns:
        PromptTemplate: A PromptTemplate object initialized with the given template and input variables.
    """
    question_answer_from_context_prompt = PromptTemplate(
        template=template,
        input_variables=input_variables,
    )
    return question_answer_from_context_prompt

def respond_to_question_using_context(question, context, question_answer_from_context_chain):
    """
    Answers a question using the specified context by invoking a reasoning chain.

    Args:
        question (str): The question to answer.
        context (str): The context information used to answer the question.
        question_answer_from_context_chain (langchain chain): A langchain chain for processing the input data and retrieving an answer.

    Returns:
        dict: A dictionary containing the answer, context, and question.
    """
    input_data = {
        "question": question,
        "context": context
    }
   
    output = question_answer_from_context_chain.invoke(input_data)
    answer = output.answer_based_on_content
    return {"answer": answer, "context": context, "question": question}

def build_eval_test_cases(
    questions: List[str],
    gt_answers: List[str],
    generated_answers: List[str],
    retrieved_documents: List[str]
) -> List[LLMTestCase]:
    """
    Builds a list of test cases for evaluating the generated answers.

    Args:
        questions (List[str]): List of questions to be tested.
        gt_answers (List[str]): List of ground-truth answers corresponding to each question.
        generated_answers (List[str]): List of answers generated by the language model for each question.
        retrieved_documents (List[str]): List of documents or contexts retrieved for each question.

    Returns:
        List[LLMTestCase]: A list of LLMTestCase objects, each containing an input question, expected answer, actual answer, and retrieved context.
    """
    return [
        LLMTestCase(
            input=question,
            expected_output=gt_answer,
            actual_output=generated_answer, # prediction
            retrieval_context=retrieved_document
        )
        for question, gt_answer, generated_answer, retrieved_document in zip(
            questions, gt_answers, generated_answers, retrieved_documents
        )
    ]

def create_context(questions, chunks_query_retriever, llm, use_hyde):
    """
    Generates answers and retrieves context documents for each question.

    Args:
        questions (List[str]): List of questions to retrieve context for.
        chunks_query_retriever (FAISS): index used to retrieve context for each question.
        llm (ChatOpenAI): The ChatOpenAI model instance used to generate hypothetical documents if use_hyde is True.
        use_hyde (bool): Whether to use the Hyde method
    Returns:
       List[Tuple[str, str]]: A list of tuples, each containing a question and its associated retrieved context.
    """
    info = []
    # Generate answers and retrieve documents for each question
    for question in questions:
        if use_hyde:
            context, hypothetical_doc = hyde(question, llm, chunks_query_retriever)
        else:
            context = retrieve_context_per_question(question, chunks_query_retriever)
        
        info.append((question,context))
    
    return info

def generate_answers(question_answer_from_context_chain, info):
    """
    Generates answers and retrieves the context documents for each question in the provided information.

    Args:
        question_answer_from_context_chain (chain): Langchain chain used to generate answers based on question and context.
        info (List[Tuple[str, str]]): List of tuples containing each question and its retrieved context.

    Returns:
        generated_answers (List): list of generated answers.
        
        retrieved_documents (List): list of retrieved documents.
    """
    generated_answers = []
    retrieved_documents = []
    for question, context in info:
        retrieved_documents.append(context)
        context_string = " ".join(context)
        result = respond_to_question_using_context(question, context_string, question_answer_from_context_chain)
        generated_answers.append(result["answer"])

    return generated_answers, retrieved_documents


def evaluate_rag(chunks_query_retriever, questions, ground_truth_answers, prompt_template, use_hyde) -> None:

    llm = create_openai_language_model()
    question_answer_from_context_chain = build_chain_with_structured_output(prompt_template, llm)

    info = create_context(questions, chunks_query_retriever, llm, use_hyde)
    generated_answers, retrieved_documents = generate_answers(question_answer_from_context_chain, info)
    # Create test cases and evaluate
    test_cases = build_eval_test_cases(questions, ground_truth_answers, generated_answers, retrieved_documents)
    evaluate(
        test_cases=test_cases,
        metrics=[correctness_metric, faithfulness_metric]
    )
